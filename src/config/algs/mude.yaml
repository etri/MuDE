# --- Qatten specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

runner: "episode"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "mude_learner"
double_q: True
mixer: "dmaq"
mixing_embed_dim: 32
hypernet_embed: 64
adv_hypernet_layers: 3
adv_hypernet_embed: 64

num_kernel: 10
is_minus_one: True
weighted_head: True
is_adv_attention: True
is_stop_gradient: True

burn_in_period: 100

name: "mude"

# MuDE parameters
q_loss: 1.0
q_loss2 : 1.0
r_loss: 1.0
r_zero_loss_plus: 1.0
r_zero_loss_minus: 1.0
Lmini: 0.001
Ldiv1: 0.01
celoss : 0.5
Decay_Ratio_Start: 0.0
Act_Decay_Start: 0
Act_Decay_End: 4000000
whole_seq: 1
subRewardDim: 32
n_out: 10
state_mask_dim: 16
modelflag: 0
cls_crietrion: 0.5
classfiydim: 32
otherlossoff: 1
delval: 0
rerror_mag: 0
initmask: 0.5